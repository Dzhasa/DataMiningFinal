# -*- coding: utf-8 -*-
"""DataMining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUxj9S08FyuT9C7Z5d6Wy0KK4UKpm7Aw
"""

#Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor, XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, mean_squared_error, r2_score, mean_absolute_error
from google.colab import drive
from imblearn.over_sampling import SMOTE

drive.mount('/content/drive')

#Load the csv (Zhalgas pls dont run it often it gets slow)
df = pd.read_csv('/content/drive/My Drive/heart_2022_with_nans.csv')

#Check its loaded
display(df.head())

#Check missing data
print(df.isnull().sum())

#Check datatypes
print(df.info())

#We are going to fill numerical columns with median and categorical columns with mode

#Define numerical and categorical
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

#Inset Numerical
num_imputer = SimpleImputer(strategy='median')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

#Insert Categorical
cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

#Check for nulls
print(df.isnull().sum())

#Remove Duplicates
df = df.drop_duplicates()

#Convert categorical data to numbers

#Encode general health into numerical
df['GeneralHealth'] = df['GeneralHealth'].map({'Excellent': 5, 'Very good': 4, 'Good': 3, 'Fair': 2, 'Poor': 1})

#Encode "Yes/No" into boolean
columns_to_encode = ['PhysicalActivities', 'HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis', 'DeafOrHardOfHearing', 'BlindOrVisionDifficulty', 'DifficultyConcentrating', 'DifficultyWalking', 'DifficultyDressingBathing', 'DifficultyErrands', 'ChestScan', 'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver', 'HighRiskLastYear']
for col in columns_to_encode:
    df[col] = df[col].map({'Yes': 1, 'No': 0})
    df[col] = df[col].astype(bool)


#'HadDiabetes', 'CovidPos' have irregular values like "No, pre-diabetes or borderline diabetes", "Yes, but only during pregnancy (female)" and for covid "Tested positive using home test without a health professional"
#и че с ними делать?

df['PhysicalHealthDays_raw'] = df['PhysicalHealthDays']
df['MentalHealthDays_raw'] = df['MentalHealthDays']

#PHYSHLTH - Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?
#MENTHLTH - Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?
#split them into several ranges by 5 days for easier display
bins = [0, 5, 10, 15, 20, 25, 30]
labels = ['1', '2', '3', '4', '5', '6']

df['PhysicalHealthDays'] = pd.cut(df['PhysicalHealthDays'],
                            bins=bins,
                            labels=labels,
                            include_lowest=True)

df['MentalHealthDays'] = pd.cut(df['MentalHealthDays'],
                            bins=bins,
                            labels=labels,
                            include_lowest=True)

#Add GDPs for states (no point in adding them as a column?)
california = 3641643
texas = 2402137
new_york = 2048403
florida = 1439065
illinois = 1025667
pennsylvania = 911813
ohio = 825990
georgia = 767378
new_jersey = 754948
north_carolina = 715968
washington = 738101
massachusetts = 691461
virginia = 663106
michigan = 622563
colorado = 491289
tennessee = 485657
maryland = 480113
arizona = 475654
indiana = 470324
minnesota = 448032
wisconsin = 396209
missouri = 396890
connecticut = 319345
oregon = 297309
south_carolina = 297546
louisiana = 291952
alabama = 281569
kentucky = 258981
utah = 256370
oklahoma = 242739
iowa = 238342
nevada = 222939
kansas = 209326
arkansas = 165989
nebraska = 164934
district_of_columbia = 165061
mississippi = 139976
new_mexico = 125541
idaho = 110871
new_hampshire = 105025
hawaii = 101083
west_virginia = 97417
delaware = 90208
maine = 85801
north_dakota = 72651
rhode_island = 72771
south_dakota = 68782
montana = 67072
alaska = 65699
wyoming = 49081
vermont = 40831
puerto_rico = 115799
guam = 6910
virgin_islands = 4672

#Add GDPs per capita for states (no point in adding them as a column?)
california_per_capita = 93460
texas_per_capita = 78750
new_york_per_capita = 104660
florida_per_capita = 63640
illinois_per_capita = 81730
pennsylvania_per_capita = 70350
ohio_per_capita = 70080
georgia_per_capita = 69570
new_jersey_per_capita = 81260
north_carolina_per_capita = 66070
washington_per_capita = 94470
massachusetts_per_capita = 98750
virginia_per_capita = 76080
michigan_per_capita = 62020
colorado_per_capita = 83580
tennessee_per_capita = 68140
maryland_per_capita = 77680
arizona_per_capita = 64010
indiana_per_capita = 68530
minnesota_per_capita = 78080
wisconsin_per_capita = 68192
missouri_per_capita = 63126
connecticut_per_capita = 88760
oregon_per_capita = 70548
south_carolina_per_capita = 56066
louisiana_per_capita = 61313
alabama_per_capita = 54753
kentucky_per_capita = 57653
utah_per_capita = 57653
oklahoma_per_capita = 59894
iowa_per_capita = 72221
nevada_per_capita = 67962
kansas_per_capita = 71729
arkansas_per_capita = 54259
nebraska_per_capita = 82207
district_of_columbia_per_capita = 242853
mississippi_per_capita = 47190
new_mexico_per_capita = 57792
idaho_per_capita = 56496
new_hampshire_per_capita = 75565
hawaii_per_capita = 68207
west_virginia_per_capita = 53852
delaware_per_capita = 85977
maine_per_capita = 61008
north_dakota_per_capita = 94021
rhode_island_per_capita = 65362
south_dakota_per_capita = 74268
montana_per_capita = 57945
alaska_per_capita = 86722
wyoming_per_capita = 81586
vermont_per_capita = 62771
puerto_rico_per_capita = 37170
guam_per_capita = 40807
virgin_islands_per_capita = 44320

# Create GDP and add it to features
gdp_map = {
    'California': 3641643, 'Texas': 2402137, 'New York': 2048403,
    'Florida': 1439065, 'Illinois': 1025667, 'Pennsylvania': 911813,
    'Ohio': 825990, 'Georgia': 767378, 'New Jersey': 754948,
    'north_carolina': 715968, 'Washington': 738101, 'Massachusetts': 691461,
    'Virginia': 663106, 'Michigan': 622563, 'Colorado': 491289,
    'Tennessee': 485657, 'Maryland': 480113, 'Arizona': 475654,
    'Indiana': 470324, 'Minnesota': 448032, 'Wisconsin': 396209,
    'Missouri': 396890, 'Connecticut': 319345, 'Oregon': 297309,
    'South Carolina': 297546, 'Louisiana': 291952, 'Alabama': 281569,
    'Kentucky': 258981, 'Utah': 256370, 'Oklahoma': 242739,
    'Iowa': 238342, 'Nevada': 222939, 'Kansas': 209326, 'Arkansas': 165989,
    'Nebraska': 164934, 'District of Columbia': 165061, 'Mississippi': 139976,
    'New Mexico': 125541, 'Idaho': 110871, 'New Hampshire': 105025,
    'hawaii': 101083, 'West Virginia': 97417, 'Delaware': 90208,
    'maine': 85801, 'North Dakota': 72651, 'Rhode Island': 72771,
    'South Dakota': 68782, 'Montana': 67072, 'Alaska': 65699,
    'Wyoming': 49081, 'Vermont': 40831, 'Puerto Rico': 115799,
    'Guam': 6910, 'Virgin Islands': 4672
}
df['GDP'] = df['State'].map(gdp_map)

#Check imbalance
#Graph for every column
for col in df.columns:
    plt.figure(figsize=(10, 4))

    if df[col].dtype in ['object', 'bool', 'category']:
        #Cathegorical columns
        value_counts = df[col].value_counts()
        plt.bar(value_counts.index.astype(str), value_counts.values)
        plt.title(f'{col} ({len(value_counts)} Value)')
        plt.xticks(rotation=45)
        plt.ylabel('Frequency')

        #Add values to the tables
        for i, v in enumerate(value_counts.values):
            plt.text(i, v, str(v), ha='center', va='bottom')

    else:
        #Numerical
        plt.hist(df[col], bins=50, edgecolor='black')
        plt.title(f'{col}')
        plt.xlabel('Amount')
        plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()
    #Describe the column
    print(f"--- {col} ---")
    print(df[col].describe())

#По сути весь препроцессинг сделан

#Ailments and heart attacks

features = [
    'RemovedTeeth', 'HadAngina', 'HadStroke', 'HadAsthma', 'HadSkinCancer',
    'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis',
    'HadDiabetes', 'DeafOrHardOfHearing', 'BlindOrVisionDifficulty',
    'DifficultyConcentrating', 'DifficultyWalking', 'DifficultyDressingBathing',
    'DifficultyErrands', 'SmokerStatus', 'ECigaretteUsage', 'ChestScan',
    'AgeCategory', 'HeightInMeters', 'WeightInKilograms', 'BMI',
    'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver',
    'TetanusLast10Tdap', 'HighRiskLastYear'
]

#sometimes it fails with bool datatype, sometimes not
df_heatmap = df.copy()
bool_cols = [col for col in features if col in df_heatmap.columns and df_heatmap[col].dtype == 'bool']
for col in bool_cols:
    df_heatmap[col] = df_heatmap[col].astype(int)
df_heatmap['HadHeartAttack'] = df_heatmap['HadHeartAttack'].astype(int)

#correlation
numeric_cols = [col for col in features + ['HadHeartAttack']
                if col in df_heatmap.columns and pd.api.types.is_numeric_dtype(df_heatmap[col])]

corr_matrix = df_heatmap[numeric_cols].corr()

#only for heart attack
heart_attack_corr = corr_matrix['HadHeartAttack'].drop('HadHeartAttack').sort_values()

# visualize
plt.figure(figsize=(12, 10))
sns.heatmap(heart_attack_corr.to_frame(),
            annot=True,
            cmap='RdBu_r',)
plt.title('Correlation of Features with Heart Attack', fontsize=14, pad=20)
plt.xlabel('HadHeartAttack')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

# Connection between HadHeartAttack and PhysicalHealthDays/MentalHealthDays

sns.boxplot(x='HadHeartAttack', y='PhysicalHealthDays_raw', data=df)
plt.title('Physical Unhealthy Days vs Heart Disease')
plt.show()

sns.boxplot(x='HadHeartAttack', y='MentalHealthDays_raw', data=df)
plt.title('Mental Unhealthy Days vs Heart Disease')
plt.show()

# Risks

risk_table = pd.crosstab(df['PhysicalHealthDays'], df['HadHeartAttack'], normalize='index') * 100
print(risk_table)

# Simple bar chart
risk_table.plot(kind='bar', figsize=(10, 5))
plt.legend(['No Heart Attack', 'Had Heart Attack'])
plt.xticks(rotation=0)
plt.show()

risk_table = pd.crosstab(df['MentalHealthDays'], df['HadHeartAttack'], normalize='index') * 100
print(risk_table)

# Simple bar chart
risk_table.plot(kind='bar', figsize=(10, 5))
plt.legend(['No Heart Attack', 'Had Heart Attack'])
plt.xticks(rotation=0)
plt.show()

# Correlation analysis

corr_cols = ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours',
             'GeneralHealth', 'HadHeartAttack']
sns.heatmap(df[corr_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

#race and heart attack correlation

race_heart = pd.crosstab(df['RaceEthnicityCategory'], df['HadHeartAttack'], normalize='index') * 100
print(race_heart)

race_heart.plot(kind='bar', figsize=(10, 5))
plt.title('Heart Attack Rate by Race')
plt.show()

#GDP and heart attack correlation

#find high GDP
df['GDP_high'] = df['GDP'] > df['GDP'].median()

gdp_heart = pd.crosstab(df['GDP_high'], df['HadHeartAttack'], normalize='index') * 100
print(gdp_heart)

gdp_heart.plot(kind='bar')
plt.title('Heart Attack Rate: High vs Low GDP States')
plt.show()

#race and GDP to show which races have access to healthcare

avg_gdp_by_race = df.groupby('RaceEthnicityCategory')['GDP'].mean().sort_values()
print(avg_gdp_by_race)

avg_gdp_by_race.plot(kind='bar', figsize=(10, 5))
plt.title('Average GDP by Race')
plt.show()

# Preprocessing and model training for predicting heart attack

y = df['HadHeartAttack']

#training features
X = df[[
    'BMI',
    'GeneralHealth',
    'PhysicalHealthDays',
    'MentalHealthDays',
    'SleepHours',
    'HadDiabetes',
    'Sex',
    'AgeCategory'
]]
#training 80%, testing 20%
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

numeric_features = [
    'BMI', 'GeneralHealth', 'PhysicalHealthDays',
    'MentalHealthDays', 'SleepHours'
]

categorical_features = [
    'HadDiabetes', 'Sex', 'AgeCategory'
]

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

#transformer for categoricak
categorical_transformer = OneHotEncoder(drop='first')

#combine transformers
preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

#transform data
X_train_transformed = preprocess.fit_transform(X_train)
X_test_transformed = preprocess.transform(X_test)

#smote to reduce imbalance
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train_transformed, y_train)

# For Logistic Regression

model = LogisticRegression(max_iter=500)

#train
model.fit(X_train_bal, y_train_bal)

#make predictions
y_pred = model.predict(X_test_transformed)
y_prob = model.predict_proba(X_test_transformed)[:, 1]

print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_prob))

# For XGB Classifier

#calculate weight for minority to reduce imbalance
scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]

#parameters for xgb
xgb = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)
#train
xgb.fit(X_train_transformed, y_train)
y_prob = xgb.predict_proba(X_test_transformed)[:,1]

#find optimal threshold using precision-recall curve
from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_test, y_prob)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]

#make predictions using optimal threshold instead of default 0.5
y_pred_opt = (y_prob >= best_threshold).astype(int)
print(classification_report(y_test, y_pred_opt))
print("ROC-AUC:", roc_auc_score(y_test, y_prob))

from collections import Counter
counts = Counter(df['State'])
print(counts)

#copy of BMI but with definitely no NaNs
df_clean = df.dropna(subset=['BMI']).copy()

# Preprocessing and model training with GDP feature for predicting BMI

numeric_features = [
    'PhysicalHealthDays',
    'MentalHealthDays',
    'SleepHours',
    'GDP'
]

categorical_features = [
    'Sex',
    'GeneralHealth',
    'AgeCategory',
    'HadDiabetes',
    'PhysicalActivities',
    'SmokerStatus',
    'AlcoholDrinkers',
    'HadHeartAttack',
    'HadStroke',
    'HadAsthma',
    'HadDepressiveDisorder',
    'HadKidneyDisease',
    'HadArthritis',
    'DifficultyWalking',
    'RaceEthnicityCategory'
]

feature_columns = numeric_features + categorical_features
df_model = df_clean[feature_columns + ['BMI']].dropna()

X = df_model[feature_columns]
y = df_model['BMI']

print(f"Data after cleaning: {len(X)} строк")
print(f"Distribute BMI: min={y.min():.1f}, max={y.max():.1f}, mean={y.mean():.1f}")

#split 20% for taining and 80 for teaching model
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Preprocessing
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Use few models for differences

models = {
    'XGBoost': XGBRegressor(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    ),
    'Random Forest': RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        random_state=42,
        n_jobs=-1
    )
}

best_model = None
best_r2 = -np.inf

for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    pipeline.fit(X_train, y_train)

    # Predict
    y_pred = pipeline.predict(X_test)

    # Metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\n{name}:")
    print(f"  Root Mean Square Error: {rmse:.2f}")
    print(f"  Mean Absolute Error: {mae:.2f}")
    print(f"  R² Score: {r2:.4f}")

    if r2 > best_r2:
        best_r2 = r2
        best_model = (name, pipeline)

print("\n" + "="*60)
print(f"Best model: {best_model[0]} с R² = {best_r2:.4f}")
print("="*60)

# Feature importance для лучшей модели (если это tree-based)
if hasattr(best_model[1].named_steps['model'], 'feature_importances_'):
    # Получаем названия признаков после one-hot encoding
    feature_names = (numeric_features +
                    list(best_model[1].named_steps['preprocessor']
                         .named_transformers_['cat']
                         .get_feature_names_out(categorical_features)))

    importances = best_model[1].named_steps['model'].feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)

    print("\nТоп-15 важных признаков:")
    print(feature_importance.head(15))

top_features = feature_importance.head(10)

plt.figure(figsize=(8, 5))
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel('Importance')
plt.title('Top 10 Features for BMI Prediction')
plt.tight_layout()
plt.show()

# RMSE (Root Mean Square Error): В среднем модель ошибается на ±5.65 единиц BMI, то есть модель предсказал реальное значение на ±5.65 единиц, что может петепутать категорию
#

